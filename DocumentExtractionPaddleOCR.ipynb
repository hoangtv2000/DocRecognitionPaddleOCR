{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end Document analysis by PadddleOCR\n",
    "\n",
    "# 1. Document recognition\n",
    "\n",
    "From images of document => To structural files \n",
    "\n",
    "Title & text contents are in .csv files, tables are in .xlsx files, cropped figures are in .jpg files. \n",
    "\n",
    "Does not support OCR to figures.\n",
    "\n",
    "\n",
    "### 1.1. Layout Parser (PaddeDetection model)\n",
    "\n",
    "Given an input image, Layout parser model can detect text, title and figure regions. \n",
    "\n",
    "\n",
    "### 1.2. Text Detection & Text Recognition\n",
    "\n",
    "Given cropped images containing text regions as input, Text Detection model detects bounding boxes of text lines.\n",
    "\n",
    "Next, Text Recognition model recognizes text => .csv files.\n",
    "\n",
    "\n",
    "### 1.3. Table Structure Recognition \n",
    "\n",
    "Given cropped tables, this model parse table's structure and recognizes text on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import layoutparser as lp\n",
    "\n",
    "model = lp.PaddleDetectionLayoutModel(config_path=\"lp://PubLayNet/ppyolov2_r50vd_dcn_365e_publaynet/config\",\n",
    "                                threshold=0.5,\n",
    "                                label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"},\n",
    "                                enforce_cpu=False,\n",
    "                                enable_mkldnn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddleocr import PaddleOCR, draw_ocr\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_content(content_blocks, target_folder, image):\n",
    "    text_img = cv2.transpose(image)\n",
    "    text_img_list = []\n",
    "\n",
    "    for idx in range(len(content_blocks)):\n",
    "        x_1, y_1, x_2, y_2 = content_blocks[idx][0]\n",
    "        text_image = cv2.transpose(text_img[int(x_1):int(x_2), int(y_1):int(y_2)])\n",
    "        type_of_text = content_blocks[idx][1]\n",
    "\n",
    "        text_img_list.append([text_image, type_of_text])\n",
    "        if not os.path.exists(str(image_path.split('.')[0] + f'/{target_folder}')):\n",
    "            os.makedirs(str(image_path.split('.')[0] + f'/{target_folder}')) \n",
    "            \n",
    "        cv2.imwrite(str(image_path.split('.')[0] + f'/{target_folder}/{idx}.jpg'), text_image)\n",
    "\n",
    "    ocr = PaddleOCR(use_angle_cls=True, lang='en') \n",
    "    text_results = [(ocr.ocr(text_img, cls=True), type_of_text) for text_img, type_of_text in (text_img_list)]\n",
    "\n",
    "    return text_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\PaddleOCR\n"
     ]
    }
   ],
   "source": [
    "%cd C:/Users/ASUS/PaddleOCR\n",
    "\n",
    "# detect\n",
    "image_path = 'C:/Users/ASUS/OneDrive/Desktop/paper/0004.jpg'\n",
    "\n",
    "def end2end_process_doc_recognition(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = image[..., ::-1]\n",
    "\n",
    "    layout = model.detect(image)\n",
    "\n",
    "    \"\"\"Text Region Recognition\n",
    "    \"\"\"\n",
    "    text_blocks = lp.Layout([b for b in layout if b.type=='Text'])\n",
    "    figure_blocks = lp.Layout([b for b in layout if b.type=='Figure'])\n",
    "    title_blocks = lp.Layout([b for b in layout if b.type=='Title'])\n",
    "    table_blocks = lp.Layout([b for b in layout if b.type=='Table'])\n",
    "\n",
    "    # text areas may be detected within the image area, delete these areas\n",
    "    text_blocks = lp.Layout([b for b in text_blocks if not any(b.is_in(b_fig) for b_fig in figure_blocks)])\n",
    "\n",
    "    # sort text areas and assign ID\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    left_interval = lp.Interval(0, w/2*1.05, axis='x').put_on_canvas(image)\n",
    "\n",
    "    left_blocks = text_blocks.filter_by(left_interval, center=True)\n",
    "    left_blocks.sort(key = lambda b:b.coordinates[1])\n",
    "\n",
    "    right_blocks = [b for b in text_blocks if b not in left_blocks]\n",
    "    right_blocks.sort(key = lambda b:b.coordinates[1])\n",
    "\n",
    "    # the two lists are merged and the indexes are added in order\n",
    "    text_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])\n",
    "\n",
    "    # assemble text and title boxes\n",
    "    text_title_blocks = text_blocks + title_blocks\n",
    "    text_title_blocks_ =  [(text_title_blocks[i].coordinates, text_title_blocks[i].type) for i in range(len(text_title_blocks))]\n",
    "    sorted_text_title_blocks = sorted(text_title_blocks_, key=lambda x: (x[0][1], x[0][0]), reverse=False)\n",
    "\n",
    "\n",
    "    \"\"\"Table Recognition\n",
    "    \"\"\"\n",
    "    box_img = cv2.transpose(image)\n",
    "    box_img = cv2.cvtColor(box_img, cv2.COLOR_BGR2RGB)\n",
    "    box_img_list = []\n",
    "\n",
    "\n",
    "    for idx in range(len(table_blocks)):\n",
    "        x_1, y_1, x_2, y_2 = table_blocks[idx].coordinates\n",
    "        box_img = cv2.transpose(box_img[int(x_1):int(x_2), int(y_1):int(y_2)])\n",
    "        box_img_list.append(box_img)\n",
    "        cv2.imwrite(str(image_path.split('.')[0] + f'/table_results/{idx}.jpg'), box_img)\n",
    "\n",
    "    # Text recognizer in table\n",
    "    from paddleocr import PPStructure, save_structure_res\n",
    "    table_engine = PPStructure(layout=False, show_log=True)\n",
    "\n",
    "    results = [table_engine(img) for img in  box_img_list]\n",
    "\n",
    "    if not os.path.exists(str(image_path.split('.')[0] + f'/table_results')):\n",
    "        os.makedirs(str(image_path.split('.')[0] + f'/table_results'))\n",
    "\n",
    "    for result in results:\n",
    "        save_structure_res(result, str(image_path.split('.')[0] + f'/table_results/'), os.path.basename(image_path).split('.')[0])\n",
    "    \n",
    "\n",
    "    \"\"\"Text & Title Recognition\n",
    "    \"\"\"\n",
    "    text_title_results = extract_content(content_blocks=sorted_text_title_blocks, target_folder='text_title_results', image=image)\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['line', 'content', 'conf'])\n",
    "\n",
    "    for idx, (result, type_of_text) in enumerate(text_title_results):\n",
    "        img_draw = cv2.imread(str(image_path.split('.')[0] + f'/text_title_results/{idx}.jpg'))\n",
    "        \n",
    "        boxes = [line[0] for line in result]\n",
    "        txts = [line[1][0] for line in result]\n",
    "        scores = [line[1][1] for line in result]\n",
    "        \n",
    "        im_show = draw_ocr(img_draw, boxes, txts, scores, font_path='doc/fonts/simfang.ttf')\n",
    "        im_show = Image.fromarray(im_show)\n",
    "\n",
    "        for row in range(len(result)):\n",
    "            row_dct = {'line': str(f'{idx}_{row}_{type_of_text}'), 'content': txts[row], 'conf': scores[row]}\n",
    "            row_dct = pd.DataFrame([row_dct])\n",
    "            result_df = pd.concat([result_df, row_dct])\n",
    "        \n",
    "        im_show.save(str(image_path.split('.')[0] + f'/text_title_results/result_{idx}.jpg'))\n",
    "\n",
    "    result_df.to_csv(str(image_path.split('.')[0] + f'/text_data.csv'), index=False)\n",
    "\n",
    "    \"\"\"Figure recognition\n",
    "    \"\"\"\n",
    "    fig_img = cv2.transpose(image)\n",
    "    fig_img = cv2.cvtColor(fig_img, cv2.COLOR_BGR2RGB)\n",
    "    figure_img_list = []\n",
    "\n",
    "\n",
    "    for idx in range(len(figure_blocks)):\n",
    "        x_1, y_1, x_2, y_2 = figure_blocks[idx].coordinates\n",
    "        fig_img_ = cv2.transpose(fig_img[int(x_1):int(x_2), int(y_1):int(y_2)])\n",
    "        figure_img_list.append(fig_img_)\n",
    "        if not os.path.exists(str(image_path.split('.')[0] + f'/figure_results')):\n",
    "            os.makedirs(str(image_path.split('.')[0] + f'/figure_results')) \n",
    "            \n",
    "        cv2.imwrite(str(image_path.split('.')[0] + f'/figure_results/{idx}.jpg'), fig_img_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end2end_process_doc_recognition(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Post-processing recognized content\n",
    "## 2.1. Concatenate lines of paragraphs and connect content of all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "paper_csv_file = pd.DataFrame(columns=['line', 'content', 'conf'])\n",
    "\n",
    "for i in range(1, 31):\n",
    "    csv_path = 'C:/Users/ASUS/OneDrive/Desktop/paper/{0:04d}/text_data.csv'.format(i)\n",
    "    csv_file = pd.read_csv(csv_path)\n",
    "    paper_csv_file = pd.concat([paper_csv_file, csv_file])\n",
    "\n",
    "paper_csv_file.reset_index(inplace=True)\n",
    "del paper_csv_file['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_csv_file = pd.DataFrame(columns=['content', 'type'])\n",
    "prev_line_idx = 0\n",
    "processed_line_idx = -1\n",
    "\n",
    "for total_index in range(paper_csv_file.shape[0]):\n",
    "    parag_idx, line_idx, type_ = paper_csv_file['line'][total_index].split('_')\n",
    "    parag_idx = int(parag_idx)\n",
    "    line_idx = int(line_idx)\n",
    "\n",
    "    if (parag_idx <= prev_line_idx) and (total_index != 0) and (type_ == processed_csv_file['type'][processed_line_idx]): \n",
    "        processed_csv_file['content'][processed_line_idx] =  \\\n",
    "                str(processed_csv_file['content'][processed_line_idx] + \" \" + paper_csv_file['content'][total_index])\n",
    "    \n",
    "    else:\n",
    "        line_dict = {'content': paper_csv_file['content'][total_index], 'type': type_}\n",
    "        line_dict = pd.DataFrame([line_dict])\n",
    "        processed_csv_file = processed_csv_file.append(line_dict, ignore_index=True)\n",
    "        processed_line_idx += 1\n",
    "\n",
    "    prev_line_idx = parag_idx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Syntax and Grammar Correction (in context of a complete sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "for idx in range(len(processed_csv_file)):\n",
    "    gfg = TextBlob(processed_csv_file.content[idx])\n",
    "    gfg = gfg.correct()\n",
    "\n",
    "    processed_csv_file.content[idx] = str(gfg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gingerit.gingerit import GingerIt\n",
    "\n",
    "\n",
    "def syntax_grammar_correction_paragraph(processed_csv_file):\n",
    "    for idx in range(len(processed_csv_file)):\n",
    "        doc = processed_csv_file['content'][idx]\n",
    "        word_list = re.findall(\"[a-zA-Z,.0-9!@#$%^&*-_+=~/]+\", doc)\n",
    "        updated_doc = \" \".join(word_list)\n",
    "\n",
    "        split_sentences = updated_doc.split('. ')\n",
    "        corrected_sentences = list()\n",
    "        for sentence in split_sentences:\n",
    "            corrected_text = GingerIt().parse(sentence)\n",
    "            corrected_sentences.append(corrected_text['result'])\n",
    "\n",
    "        corrected_paragraph = \". \".join(corrected_sentences)\n",
    "        processed_csv_file['content'][idx] = corrected_paragraph\n",
    "    \n",
    "    return processed_csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_csv_file = syntax_grammar_correction_paragraph(processed_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5726bf22bfc3923536ebd566fe9142349541131173315435457c0384c768932c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf-gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
